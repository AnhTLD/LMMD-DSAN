{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ayf5B7brI2l-"},"outputs":[],"source":["# Link Driver\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRBztvE_UUbl"},"outputs":[],"source":["root_path = '/content/drive/MyDrive/HUST/'\n","src = 'Data/E07-1/'\n","tar = 'Data/E21-3/'\n","\n","tar_train = 'Train'\n","tar_val = 'Validation '\n","tar_test = \"Test\"\n","\n","nclass = 4 #số lớp phân biệt\n","batch_size = 64\n","nepoch = 50\n","lr = [0.0001, 0.001, 0.01]\n","early_stop = 5\n","seed = 2021\n","weight = 0.5\n","momentum = 0.9\n","decay = 5e-4\n","bottleneck = True\n","\n","version = src[-6:-1] + ' - ' + tar[-6:-1] + ' weight = ' +str(weight)\n","print(version)"]},{"cell_type":"markdown","metadata":{"id":"QqIqhx3z5dA2"},"source":["MODEL"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"cSm4fL2ZHdpd"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torchvision'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"]}],"source":["import torch\n","import torch.nn as nn\n","import math\n","import torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERjtPh8f5FTQ"},"outputs":[],"source":["class VGG16(nn.Module):\n","    def __init__(self, num_classes=1000):\n","        super(VGG16, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yN6K53pZ5FPC"},"outputs":[],"source":["def model(pretrained=False, **kwargs):\n","    model = VGG16(**kwargs)\n","    if pretrained:\n","        model.load_state_dict(torchvision.models.vgg16(pretrained=True).state_dict())\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"r0mWATKc5kdC"},"source":["Local Maximum Mean Discrepancy (LMMD)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wTbY020d5aUb"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sN9bB_hgHkbE"},"outputs":[],"source":["class LMMD_loss(nn.Module):\n","    def __init__(self, class_num = nclass, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n","        super(LMMD_loss, self).__init__()\n","        self.class_num = class_num\n","        self.kernel_num = kernel_num\n","        self.kernel_mul = kernel_mul\n","        self.fix_sigma = fix_sigma\n","        self.kernel_type = kernel_type\n","\n","    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n","        n_samples = int(source.size()[0]) + int(target.size()[0])\n","        total = torch.cat([source, target], dim=0)\n","        total0 = total.unsqueeze(0).expand(\n","            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n","        total1 = total.unsqueeze(1).expand(\n","            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n","        L2_distance = ((total0-total1)**2).sum(2)\n","        if fix_sigma:\n","            bandwidth = fix_sigma\n","        else:\n","            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n","        bandwidth /= kernel_mul ** (kernel_num // 2)\n","        bandwidth_list = [bandwidth * (kernel_mul**i)\n","                          for i in range(kernel_num)]\n","        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n","                      for bandwidth_temp in bandwidth_list]\n","        return sum(kernel_val)\n","\n","    def get_loss(self, source, target, s_label, t_label):\n","        batch_size = source.size()[0]\n","        weight_ss, weight_tt, weight_st = self.cal_weight(s_label,\n","                                                          t_label,\n","                                                          batch_size=batch_size,\n","                                                          class_num=self.class_num)\n","        weight_ss = torch.from_numpy(weight_ss).cuda()\n","        weight_tt = torch.from_numpy(weight_tt).cuda()\n","        weight_st = torch.from_numpy(weight_st).cuda()\n","\n","        kernels = self.guassian_kernel(source,\n","                                       target,\n","                                       kernel_mul=self.kernel_mul,\n","                                       kernel_num=self.kernel_num,\n","                                       fix_sigma=self.fix_sigma)\n","        loss = torch.Tensor([0]).cuda()\n","        if torch.sum(torch.isnan(sum(kernels))):\n","            return loss\n","        SS = kernels[:batch_size, :batch_size]\n","        TT = kernels[batch_size:, batch_size:]\n","        ST = kernels[:batch_size, batch_size:]\n","\n","        loss += torch.sum(weight_ss * SS + weight_tt * TT - 2 * weight_st * ST)\n","        return loss\n","\n","    def convert_to_onehot(self, sca_label, class_num=nclass):\n","        return np.eye(class_num)[sca_label]\n","\n","    def cal_weight(self, s_label, t_label, batch_size=32, class_num=nclass):\n","        batch_size = s_label.size()[0]\n","        s_sca_label = s_label.cpu().data.numpy()\n","        s_vec_label = self.convert_to_onehot(s_sca_label, class_num=self.class_num)\n","        s_sum = np.sum(s_vec_label, axis=0).reshape(1, class_num)\n","        s_sum[s_sum == 0] = 100\n","        s_vec_label = s_vec_label / s_sum\n","\n","        t_sca_label = t_label.cpu().data.max(1)[1].numpy()\n","        t_vec_label = t_label.cpu().data.numpy()\n","        t_sum = np.sum(t_vec_label, axis=0).reshape(1, class_num)\n","        t_sum[t_sum == 0] = 100\n","        t_vec_label = t_vec_label / t_sum\n","\n","        index = list(set(s_sca_label) & set(t_sca_label))\n","        mask_arr = np.zeros((batch_size, class_num))\n","        mask_arr[:, index] = 1\n","        t_vec_label = t_vec_label * mask_arr\n","        s_vec_label = s_vec_label * mask_arr\n","\n","        weight_ss = np.matmul(s_vec_label, s_vec_label.T)\n","        weight_tt = np.matmul(t_vec_label, t_vec_label.T)\n","        weight_st = np.matmul(s_vec_label, t_vec_label.T)\n","\n","        length = len(index)\n","        if length != 0:\n","            weight_ss = weight_ss / length\n","            weight_tt = weight_tt / length\n","            weight_st = weight_st / length\n","        else:\n","            weight_ss = np.array([0])\n","            weight_tt = np.array([0])\n","            weight_st = np.array([0])\n","        return weight_ss.astype('float32'), weight_tt.astype('float32'), weight_st.astype('float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRffYBq76GNP"},"outputs":[],"source":["class DSAN(nn.Module):\n","    def __init__(self, num_classes=nclass, bottle_neck=True):\n","        super(DSAN, self).__init__()\n","        self.feature_layers = model(pretrained=True).features\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.flatten = nn.Flatten()\n","        self.lmmd_loss = LMMD_loss(class_num=num_classes)\n","        self.bottle_neck = bottle_neck\n","        if bottle_neck:\n","            self.bottle = nn.Linear(512 * 7 * 7, 256)\n","            self.cls_fc = nn.Linear(256, num_classes)\n","        else:\n","            self.cls_fc = nn.Linear(512 * 7 * 7, num_classes)\n","\n","    def forward(self, source, target, s_label):\n","        source = self.feature_layers(source)\n","        source = self.avgpool(source)\n","        source = self.flatten(source)\n","        if self.bottle_neck:\n","            source = self.bottle(source)\n","        s_pred = self.cls_fc(source)\n","\n","        target = self.feature_layers(target)\n","        target = self.avgpool(target)\n","        target = self.flatten(target)\n","        if self.bottle_neck:\n","            target = self.bottle(target)\n","        t_label = self.cls_fc(target)\n","\n","        loss_lmmd = self.lmmd_loss.get_loss(source, target, s_label, torch.nn.functional.softmax(t_label, dim=1))\n","\n","        return s_pred, loss_lmmd\n","\n","    def predict(self, x):\n","        x = self.feature_layers(x)\n","        x = self.avgpool(x)\n","        x = self.flatten(x)\n","        if self.bottle_neck:\n","            x = self.bottle(x)\n","        return self.cls_fc(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3g6LWW42Vx8G"},"outputs":[],"source":["model = DSAN(num_classes= nclass).cuda()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"ajv-z1ZFE0sQ"},"source":["Load data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ryGqhg1dmkEG"},"outputs":[],"source":["from torchvision import datasets, transforms\n","import os"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"b9x7elHTqaRz"},"outputs":[],"source":["import random\n","import shutil"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690954905892,"user":{"displayName":"Khôi VM","userId":"10677837094991279756"},"user_tz":-420},"id":"BLFqQ2oEqTKW","outputId":"73bb8ea1-f4e0-452f-f7cf-e7234ee98ce3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Đã có cả ba thư mục Train, Validation , và Test trong thư mục nguồn.\n"]}],"source":["def check_train_val_test_folders_exist(src_folder):\n","    train_folder = os.path.join(src_folder, tar_train)\n","    val_folder = os.path.join(src_folder, tar_val)\n","    test_folder = os.path.join(src_folder, tar_test)\n","\n","    # Kiểm tra tính tồn tại của thư mục train, val, và test\n","    if os.path.exists(train_folder) and os.path.exists(val_folder) and os.path.exists(test_folder):\n","        print('Đã có cả ba thư mục '+ tar_train +', '+ tar_val +', và '+ tar_test +' trong thư mục nguồn.')\n","        return True\n","    else:\n","        return False\n","\n","def split_data(src_folder, train_folder, val_folder, test_folder, train_ratio=0.64, val_ratio=0.16):\n","    os.makedirs(train_folder, exist_ok=True)\n","    os.makedirs(val_folder, exist_ok=True)\n","    os.makedirs(test_folder, exist_ok=True)\n","\n","    for folder_name in ['B', 'I', 'N', 'O']:\n","        os.makedirs(os.path.join(train_folder, folder_name), exist_ok=True)\n","        os.makedirs(os.path.join(val_folder, folder_name), exist_ok=True)\n","        os.makedirs(os.path.join(test_folder, folder_name), exist_ok=True)\n","\n","        file_list = os.listdir(os.path.join(src_folder, folder_name))\n","        num_files = len(file_list)\n","\n","        random.shuffle(file_list)\n","\n","        num_train = int(num_files * train_ratio)\n","        num_val = int(num_files * val_ratio)\n","\n","        train_files = file_list[:num_train]\n","        val_files = file_list[num_train:num_train+num_val]\n","        test_files = file_list[num_train+num_val:]\n","\n","        for file in train_files:\n","            src_path = os.path.join(src_folder, folder_name, file)\n","            dest_path = os.path.join(train_folder, folder_name, file)\n","            shutil.copy(src_path, dest_path)\n","\n","        for file in val_files:\n","            src_path = os.path.join(src_folder, folder_name, file)\n","            dest_path = os.path.join(val_folder, folder_name, file)\n","            shutil.copy(src_path, dest_path)\n","\n","        for file in test_files:\n","            src_path = os.path.join(src_folder, folder_name, file)\n","            dest_path = os.path.join(test_folder, folder_name, file)\n","            shutil.copy(src_path, dest_path)\n","\n","src_folder = root_path + tar\n","\n","if not check_train_val_test_folders_exist(src_folder):\n","    train_folder = os.path.join(src_folder, tar_train)\n","    val_folder = os.path.join(src_folder, tar_val)\n","    test_folder = os.path.join(src_folder, tar_test)\n","    split_data(src_folder, train_folder, val_folder, test_folder, train_ratio=0.8, val_ratio=0)\n","\n","\n","tar_train = tar + tar_train\n","tar_val = tar + tar_val\n","tar_test = tar + tar_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uFyPzcIDjV9"},"outputs":[],"source":["def rename_folders(folder_path):\n","    folders = ['B', 'I', 'N', 'O']\n","    new_names = ['2', '3', '1', '4']\n","\n","    for folder, new_name in zip(folders, new_names):\n","        old_folder_path = os.path.join(folder_path, folder)\n","        new_folder_path = os.path.join(folder_path, new_name)\n","\n","        if os.path.exists(old_folder_path):\n","            os.rename(old_folder_path, new_folder_path)\n","            print(f'Renamed \"{folder}\" to \"{new_name}\"')\n","        else:\n","            print(f'Folder \"{folder}\" not found in \"{folder_path}\".')\n","\n","    renamed_folders = os.listdir(folder_path)\n","    print(f'Updated folder list: {renamed_folders}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWwSSQVkwiL6"},"outputs":[],"source":["folders = ['B', 'I', 'N', 'O']\n","new_names = ['2', '3', '1', '4']\n","\n","rename_folders(root_path + tar_train)\n","rename_folders(root_path + tar_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1SLeqiDG-Rr"},"outputs":[],"source":["def load_training(root_path, dir, batch_size, kwargs):\n","    transform = transforms.Compose(\n","        [transforms.Resize([256, 256]),\n","         #transforms.RandomCrop(224),\n","         #transforms.RandomHorizontalFlip(),\n","         transforms.ToTensor()])\n","    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n","    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n","    return train_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHl3MQb4p-Gh"},"outputs":[],"source":["def load_testing(root_path, dir, batch_size, kwargs):\n","    transform = transforms.Compose(\n","        [transforms.Resize([256, 256]),\n","         transforms.ToTensor()])\n","    data = datasets.ImageFolder(root=root_path + dir, transform=transform)\n","    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, **kwargs)\n","    return test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B8G3H2V6J6j"},"outputs":[],"source":["def load_data(root_path, src, tar_train, tar_test, batch_size):\n","    kwargs = {'num_workers': 1, 'pin_memory': True}\n","    loader_src = load_training(root_path, src, batch_size, kwargs)\n","    loader_tar = load_training(root_path, tar_train, batch_size, kwargs)\n","    loader_tar_test = load_testing(root_path, tar_test, batch_size, kwargs)\n","    return loader_src, loader_tar, loader_tar_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVsLZWKrHxBE"},"outputs":[],"source":["dataloaders = load_data(root_path, src, tar_val , tar_val, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDZAeRK4FB-x"},"outputs":[],"source":["training_history =[\n","    ['epoch','Loss','lossCLS','lossLMMD','Accuracy'],\n","    ['Source', src ,'---','Target',tar]\n","]\n","print(training_history)"]},{"cell_type":"markdown","metadata":{"id":"KX_GebvZE1Hf"},"source":["Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtJpuX3izkJ7"},"outputs":[],"source":["import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0XxhNWl6NeE"},"outputs":[],"source":["def train_epoch(epoch, model, dataloaders, optimizer):\n","    model.train()\n","    source_loader, target_train_loader, _ = dataloaders\n","    iter_source = iter(source_loader)\n","    iter_target = iter(target_train_loader)\n","    num_iter = len(source_loader)\n","\n","    for i, (data_source, label_source) in enumerate(iter_source):\n","        data_target, _ = next(iter_target)\n","        if i % len(target_train_loader) == 0:\n","            iter_target = iter(target_train_loader)\n","        data_source, label_source = data_source.cuda(), label_source.cuda()\n","        data_target = data_target.cuda()\n","\n","        optimizer.zero_grad()\n","        label_source_pred, loss_lmmd = model(data_source, data_target, label_source)\n","        loss_cls = F.nll_loss(F.log_softmax(label_source_pred, dim=1), label_source)\n","        lambd = 2 / (1 + math.exp(-10 * (epoch) / nepoch)) - 1\n","        loss = loss_cls + weight * lambd * loss_lmmd\n","\n","        loss.backward()\n","        optimizer.step()\n","        print(f'Epoch: [{epoch:d}], Loss: {loss.item():.4f}, cls_Loss: {loss_cls.item():.4f}, loss_lmmd: {loss_lmmd.item():.4f}')\n","        training_history.append([float(str(epoch) + '.' + str(i)),loss.item(),loss_cls.item(),loss_lmmd.item(),'----'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flct-ENXd9Li"},"outputs":[],"source":["def test(model, dataloader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in dataloader:\n","            data, target = data.cuda(), target.cuda()\n","            pred = model.predict(data)\n","            test_loss += F.nll_loss(F.log_softmax(pred, dim=1), target).item()\n","            pred = pred.data.max(1)[1]\n","            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","        test_loss /= len(dataloader)\n","        print(f'Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(dataloader.dataset)} ({100. * correct / len(dataloader.dataset):.2f}%)')\n","        training_history[-1][-1] = 100. * correct / len(dataloader.dataset)\n","        return correct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ka_sNh9oEgn3"},"outputs":[],"source":["if __name__ == '__main__':\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    correct = 0\n","    stop = 0\n","    best_loss = float('inf')\n","    if bottleneck:\n","        optimizer = torch.optim.SGD([\n","            {'params': model.feature_layers.parameters()},\n","            {'params': model.bottle.parameters(), 'lr': lr[1]},\n","            {'params': model.cls_fc.parameters(), 'lr': lr[2]},\n","        ], lr=lr[0], momentum=momentum, weight_decay=decay)\n","    else:\n","        optimizer = torch.optim.SGD([\n","            {'params': model.feature_layers.parameters()},\n","            {'params': model.cls_fc.parameters(), 'lr': lr[1]},\n","        ], lr=lr[0], momentum=momentum, weight_decay=decay)\n","\n","    for epoch in range(1, nepoch):\n","        stop += 1\n","        for index, param_group in enumerate(optimizer.param_groups):\n","            param_group['lr'] = lr[index] / math.pow((1 + 10 * (epoch - 1) / nepoch), 0.75)\n","        train_epoch(epoch, model, dataloaders, optimizer)\n","\n","\n","        current_loss = training_history[1][-1]\n","        if current_loss < best_loss:\n","            best_loss = current_loss\n","            stop = 0\n","        else:\n","            stop += 1\n","\n","        if stop >= early_stop:\n","            print(f'Loss has not improved for {early_stop} consecutive epochs. Stopping training.')\n","            break\n","\n","        # t_correct = test(model, dataloaders[-1])\n","\n","        # if t_correct > correct:\n","        #     correct = t_correct\n","        #     stop = 0\n","\n","        # print(f'{src}-{tar}: max correct: {correct} max accuracy: {100. * correct / len(dataloaders[-1].dataset):.2f}%\\n')\n","\n","        # if stop >= early_stop:\n","        #     print(f'Final test acc: {100. * correct / len(dataloaders[-1].dataset):.2f}%')\n","        #     training_history.append(['Final','Best','accuracy',100. * correct / len(dataloaders[-1].dataset)])\n","        #     break"]},{"cell_type":"markdown","metadata":{"id":"uwmBTicGcHF9"},"source":["Save model and logging data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JMn_XKHGxdG"},"outputs":[],"source":["os.makedirs(root_path + 'Save/'+version, exist_ok=True)\n","torch.save(model, root_path + 'Save/'+version+'/model_VGG.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bMRtiVwYm_w"},"outputs":[],"source":["import csv\n","csv_file = root_path + 'Save/'+version+'/training_history.csv'\n","with open(csv_file, 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerows(training_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNlmmQn2L7g1"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REHbWl7RG0Ja"},"outputs":[],"source":["epochs = [0]\n","for i in range(1, len([row[0] for row in training_history[2:-1]])):\n","    new_value = epochs[-1] + 1\n","    epochs.append(new_value)\n","\n","loss = [row[1] for row in training_history[2:-1]]\n","lossCLS = [row[2] for row in training_history[2:-1]]\n","lossLMMD = [row[3] for row in training_history[2:-1]]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, loss, label='Loss', marker='')\n","plt.plot(epochs, lossCLS, label='lossCLS', marker='')\n","plt.plot(epochs, lossLMMD, label='lossLMMD', marker='')\n","plt.xlabel('Epoch')\n","plt.xticks([])\n","plt.ylabel('Loss')\n","plt.title('Training History')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.savefig(root_path + 'Save/'+version+'/Training History.png', bbox_inches='tight')\n","plt.show()\n","plt.clf()"]},{"cell_type":"markdown","metadata":{"id":"oWxHmUcFzzIX"},"source":["Testting before training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3JceCw9I_jG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import datasets, transforms\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7yIchx3NMIf"},"outputs":[],"source":["def predict_image(image_path, model_path):\n","    model = torch.load(model_path, map_location=torch.device('cuda'))\n","    model = model.to('cuda')\n","    model.eval()\n","\n","    transform = transforms.Compose([\n","        transforms.Resize([256, 256]),\n","        transforms.ToTensor()\n","    ])\n","    image = Image.open(image_path)\n","    image = image.convert('RGB')\n","    input_tensor = transform(image)\n","    input_batch = input_tensor.unsqueeze(0).to('cuda')\n","\n","    with torch.no_grad():\n","        output, _ = model(input_batch, input_batch, torch.tensor([0]).to('cuda'))\n","    output = output.cpu()\n","    return output\n","\n","\n","source_path = root_path + src\n","target_path = root_path + tar_test\n","\n","model_path = root_path + 'Save/'+version+'/model_VGG.pkl'\n","def load_data(image_folder):\n","    valid_labels = ['B', 'I', 'N', 'O']\n","    true = []\n","    pred = []\n","    embeddings = []\n","    labels = []\n","\n","    for folder in os.listdir(image_folder):\n","        folder_path = os.path.join(image_folder, folder)\n","        if os.path.isdir(folder_path) and folder[0] in valid_labels:\n","            print(f'Processing images in folder: {folder}')\n","            true_label = folder[0]\n","\n","            for file in os.listdir(folder_path):\n","                if file.endswith(\".jpg\") or file.endswith(\".png\"):\n","                    file_path = os.path.join(folder_path, file)\n","                    output = predict_image(file_path, model_path)\n","\n","                    _, predicted_idx = torch.max(output, 1)\n","                    predicted_label = predicted_idx.item()\n","\n","                    if true_label == 'B':\n","                        true_label = int(0)\n","                    elif true_label == 'I':\n","                        true_label = int(1)\n","                    elif true_label == 'N':\n","                        true_label = int(2)\n","                    elif true_label == 'O':\n","                        true_label = int(3)\n","\n","                    pred.append(predicted_label)\n","                    true.append(true_label)\n","\n","                    embeddings.append(output.squeeze().cpu().numpy())\n","                    labels.append(predicted_label)\n","\n","    return embeddings, labels, pred, true"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OfDT5omsbtX3"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"]}],"source":["import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCIQ3WwU7aki"},"outputs":[],"source":["print('Load source')\n","source_embeddings, source_labels, x_pred, x_true = load_data(source_path)\n","print('Load target')\n","target_embeddings, target_labels, y_pred, y_true = load_data(target_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUUTlw7OjFMC"},"outputs":[],"source":["accuracy = accuracy_score(y_true , y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Accuracy:\", accuracy*100)\n","print(\"F1 Score:\", f1*100)\n","print(\"Precision:\", precision*100)\n","print(\"Recall:\", recall*100)\n","\n","file_path = root_path + 'Save/'+ version +'/test.txt'\n","with open(file_path, \"w\") as file:\n","    file.write(\"Accuracy: {:.4f}\\n\".format(accuracy*100))\n","    file.write(\"F1 Score: {:.4f}\\n\".format(f1*100))\n","    file.write(\"Precision: {:.4f}\\n\".format(precision*100))\n","    file.write(\"Recall: {:.4f}\\n\".format(recall*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2XA1gbeblpX"},"outputs":[],"source":["y_true = np.array(y_true)\n","y_pred = np.array(y_pred)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['B', 'I', 'N', 'O'], yticklabels=['B', 'I', 'N', 'O'])\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Target Confusion Matrix')\n","\n","plt.savefig(root_path + 'Save/'+version+'/Target Confusion Matrix.png')\n","plt.show()\n","plt.clf()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QqScAaRUh7Xf"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'plotly'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"]}],"source":["from sklearn.manifold import TSNE\n","import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09CUNYoxsFaC"},"outputs":[],"source":["source_embeddings = np.array(source_embeddings)\n","source_labels = np.array(source_labels)\n","\n","target_embeddings = np.array(target_embeddings)\n","target_labels = np.array(target_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Htsr9Xloqlbz"},"outputs":[],"source":["tsne = TSNE(n_components=2, random_state=42)\n","\n","source_tsne_representation = tsne.fit_transform(source_embeddings)\n","target_tsne_representation = tsne.fit_transform(target_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MB26YO7zsHM"},"outputs":[],"source":["import plotly.offline as pyo\n","\n","fig = go.Figure()\n","\n","label_colors = {\n","    0: 'blue',\n","    1: 'green',\n","    2: 'red',\n","    3: 'yellow',\n","}\n","\n","source_label_map = {\n","    0: 'Source B',\n","    1: 'Source I',\n","    2: 'Source N',\n","    3: 'Source O',\n","}\n","target_label_map = {\n","    0: 'Target B',\n","    1: 'Target I',\n","    2: 'Target N',\n","    3: 'Target O',\n","}\n","\n","for label in np.unique(source_labels):\n","    new_label = source_label_map[label]\n","    label_indices = np.where(source_labels == label)[0]\n","    x = source_tsne_representation[label_indices, 0]\n","    y = source_tsne_representation[label_indices, 1]\n","    fig.add_trace(go.Scatter(\n","        x=x,\n","        y=y,\n","        mode='markers',\n","        marker=dict(symbol='x', size=10, color=label_colors[label]),\n","        showlegend=True,\n","        name=new_label\n","    ))\n","\n","for label in np.unique(target_labels):\n","    new_label = target_label_map[label]\n","    label_indices = np.where(target_labels == label)[0]\n","    x = target_tsne_representation[label_indices, 0]\n","    y = target_tsne_representation[label_indices, 1]\n","    fig.add_trace(go.Scatter(\n","        x=x,\n","        y=y,\n","        mode='markers',\n","        marker=dict(symbol='circle-open', size=8, color=label_colors[label]),\n","        showlegend=True,\n","        name=new_label\n","    ))\n","\n","fig.update_layout(\n","    title='t-SNE Visualization of Source and Target Domains',\n","    width=1000,\n","    height=800,\n","    plot_bgcolor='rgba(200, 200, 200, 0.8)'\n",")\n","\n","plot_filename = root_path + 'Save/'+ version + '/tsne_visualization.html'\n","pyo.plot(fig, filename=plot_filename, auto_open=False)\n","\n","fig.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}
